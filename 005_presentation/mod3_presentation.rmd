---
title: "Advances in the analysis of multivariate ecological data"
subtitle: "MOD3: Advanced data science"
author: "Jonathan Jupke"
institute: "University of Koblenz Landau"
date: "2021/01/07"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "libs/sydney.css", "libs/sydney-fonts.css"]
    nature:
      countIncrementalSlides: false
      navigation:
        scroll: false # disable slide transitions by scrolling

---
```{css, echo = FALSE}
.remark-code { font-family: 'Source Code Pro', 'Lucida Console', Monaco, monospace;
                                    font-size: 100%;
                                  }
}
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```




<!--TODO abstand zwischen Bullets-->  
<!--# TODO serial slide-->   
<!--# TODO bullet color-->  

# Plan for today 

- What’s new in multivariate analysis?

- mvabund

- Vector GLM/GAM

- Latent Variable Models 

- Copulas 

- Hierarchical Modeling of Species Communities

- Neural Hierarchical Models 


---
class: inverse, center, middle 

# What’s new in multivariate analysis?

---

# ... or what's old

--

brainstrom : What Methods do you know? 

--

- PCA
- RDA
- PERMANOVA
- CA/ CCA
- Random Forests 
- Clustering Methods 

---
# Bi/Triplot

```{r load vegan, results='hide', echo = T, message = F}
library(vegan)
data(dune)
data(dune.env)
rda_object <- rda(dune ~ ., dune.env)
```

---
# Bi/Triplot
```{r, fig.width = 6, fig.height = 6, fig.align="center"}
plot(rda_object)
```

---
# What do most ordinations have in common? 

--

They are based on some notion of a distance metric

##fixed 
- PCA and RDA: Euclidean 
- CA and CCA: $\chi^2$-distance

--

##flexible 
- NMDS 
- PCoA
- dbRDA

---
# How does that differ from the univerate and why? 

--

In univariate analyses we mostly use models:
- linear model 
- linear mixed model 
- generalized linear model 
- generalized additive models 

--

$\rightarrow$ why not in multivariate analyses? 

---
class: middle, center

```{r, fig.align="center", echo = F}
knitr::include_graphics("old_pc.gif")
```

---
# The idea is old 

The Gaussian response model: 
$$Y_i = ce^{- \frac{(X_i - u)^2}{2t^2}} $$
--
Can be rewritten as a GLM:
$$Y_i = exp(ln(c) - \frac{u^2}{2t^2} + \frac{u}{t^2}x_i - \frac{1}{2t^2}x^2_i) = exp(b_1 + b_2 x_i + b_3 x_i^2)$$
--
with 
$$ t = \frac{1}{\sqrt{-2b_3}}; u = \frac{-b_2}{2b_3}; c =     exp(b_1 - \frac{b_2^2}{4b_3})$$ 
--
For M species and Q environmental variables we would need: $(1+2Q)M$ parameters. For 10 species and 5 variables 110 parameters.  

---
# Restricted Gaussian Regression 

gradients as linear combinations of measured variables: 

$$z_i = \Sigma_{p =  1}^Q \alpha_p x_{ip}$$
--
Plug gradients back into Gaussian regression
$$Y_i = ce ^{\frac{(z_i - u)^2}{2t^2}}$$ 

Depending on the number of gradients this reduces the number of parameters. 

---
# What is the model-based approach? 

Model-based approaches assume a parametric generative model

Terms are not consistent:  
Roberts (2019) vs. Warton *el al.* (2012) dissagree about wether CA is distance-based

Alternative: algorithm-based (Warton *et al.* 2015) 
Alternative 2: transformation-based  

"*Our constrained ordination model [...]* " Brittain *et al.* (2020)  
"*Many biologist fit CCA models [...]* " Yee (2004) 

---
background-image: url("figures/glm_error.png")
background-size: 500px
background-position: 50% 50%
<!-- Are models better  1-->
# Are models better? 

---
background-image: url("figures/WWW12_1.png")
background-size: 800px
background-position: 50% 50%
<!-- Are models better  2-->
# Are models better? 

Every distance metric assumes a mean variance relationship  

.footnote[Warton *et al.* (2012)]
---
background-image: url("figures/WWW12_2.png")
background-size: 400px
background-position: 50% 75%

# Are models better? 

Every distance metric assumes a mean variance relationship  

.footnote[Warton *et al.* (2012)]
---
background-image: url("figures/WWW12_3.png")
background-size: 400px
background-position: 50% 75%

# Are models better? 

Every distance metric assumes a mean variance relationship  



.footnote[Warton *et al.* (2012)]
---
background-image: url("figures/szöcs15_1.png")
background-size: 400px
background-position: 50% 75%

# Are models better? 



.footnote[Szöcs *et al.* (2015)]
---
background-image: url("figures/jupke20_1.png")
background-size: 800px
background-position: 50% 50%

# Are models better? 

.footnote[Jupke *et al.* (2020)]
---
class: inverse, center, middle 

# mvabund

---
# mvabund 

R package published in 2012  (Wang et al. 2012)

**M**ulti**v**ariate **Abund**ance data  

Multivariate
- many, possibly correlated responses 

Abundance
- strong mean-variance relationship 
---
# Multivariate GLMM

We could use mixed models and model the taxon as a random effect.
$$Abundace \sim species\ + variable1\ + variable2\ + (0 + species|sample)$$
1. $y_{ij}$ are independand, conditional on mean  $\mu_{ij}$
2. Conditional on $\mu_{ij}$, data come from a .blue[known distribution]  
3. straight line relationship between some function $\mu_{ij}$ and x, with error $\epsilon_{ij}$
$$g(\mu_{ij}) = \beta_0j + x^T_j \beta_j + \epsilon_{ij}$$
4. $\epsilon$ are normally distributed. They introduce correlation between response within observatios. 
$$\epsilon_{ij} \sim MVN(0, \Sigma)$$
---
# Implementations in R

- lme4 
- MCMCglmm 
- Purpose written code (e.g. Pollock et al 2014)

---
# Whats the problem with this approach? 

|  # responses | # parameters in $\Sigma$ |
| :---- |:--- |
| 5     | 15 |
| 10    | 55 |
| 20    | 210 |
| 40    |  820 |

- No convergence in ML GLMM  
- long run time, not all parameters converge, and big influence of prior as $\frac{\# data}{\#parameters}$ is small. 
  
---
# Simplify
mvabund uses design-based inference and assumes simple correlation structures.  
This is fine if we are primarily interested in species environment relationships  

--
Multivariate: row resampling for inference, preserves species correlations  
Abundance: separate GLM for each species
---
# Test statistics  

default: $\Sigma\ Likelihood\ Ratios$
\begin{align}
L & = 2 \times (l_M - l_m)  \\
L &\sim \chi^2_{p_M - p_m} \\  
\end{align}

This does not account for correlations in the statistic but in the pseudo-*p*-value.  
--
  
  
Alternatives:      
  
Wald statistic   
\begin{align}
W &= \frac{(\beta_m - \beta_0)^2}{var(\beta_m)} \\
z &= \sqrt(W)
\end{align}

Score statistic  
see e.g. Dunn \& Smyth (2018)

---
# Correlation options
  
.blue["I"]: No correlation, default setting   
.blue["R"]: Assumes correlation, cf. mixed model  
.blue["shrink"]: middle option, shrinks correlation matrix to $I$ with ridge regularization (Warton (2008))  
  
For R and shrink LR is not available. They are estimated with generalized estimation equations (Warton 2011) which do not produce likelihoods. 

---

```{r}
library(mvabund)
data("Tasmania")
attach(Tasmania)
head(copepods[,1:3])
```
```{r}
tasmvabund <- mvabund(copepods)
class(tasmvabund)
```
---
```{r, message = FALSE, fig.align="center", warning=FALSE, fig.height=6}
plot(tasmvabund~treatment, col = as.numeric(block))
```
---
```{r, message = FALSE, warning=FALSE, fig.align="center",  fig.height=7}
meanvar.plot(copepods~tr.block, col = as.numeric (treatment))
```
---
# Fitting a model 
```{r}
tas.nb <- manyglm(copepods ~ block*treatment, 
                  family = "negative.binomial")
```


$$Y_{ij} \sim NB(\mu_{jkl}, \phi_j)$$
$$log(\mu_{jkl}) = intercept_j + block_{jkl} + treatment_{jl} + block \times treatment_{jkl}$$
---
# Checking model assumptions 
```{r mvabund Checking model assumptions 1, fig.align="center", fig.height=6, fig.width=6}
plot.manyglm(tas.nb, which = 1)
```
.footnote[Dunn & Smyth (1996)]
---
# Checking model assumptions
```{r mvabund Checking model assumptions 2, fig.align="center", fig.height=6, fig.width=6}
plot.manyglm(tas.nb, which = 2)
```
.footnote[Dunn & Smyth (1996)]
---
# Checking model assumptions
```{r mvabund Checking model assumptions 3, fig.align="center", fig.height=6, fig.width=6}
plot.manyglm(tas.nb, which = 3)
```
.footnote[Dunn & Smyth (1996)]
---
# Testing Hypotheses

```{r mvabund Testing Hypotheses, comment=""}
# anova_out <- anova(tas.nb, p.uni = "adjusted")
```
.scroll-box-16[
```{r}
# anova_out
```
]
---
```{r}
library(lattice)
a <- max(abs(coef(tas.nb)))
colort <- colorRampPalette(c("blue", "white", "red"))
plot.tas <- levelplot(t(as.matrix(coef(tas.nb))), ylab = "", xlab  = "", col.regions = colort(100), at=seq(-a,a,length = 100), scales = list( x= list(rot = 45)))
print(plot.tas)
```
---
```{r}
tas_pred = predict(tas.nb, type = "response")
matplot(t(tas_pred[c(1,3),]), type = "l", xaxt = "n", log = "y", ylab = "Mean abundance [log]")
axis(1, at =1:12, labels = colnames(copepods), las = 3)
legend("topright", legend = levels(treatment), col = 1:2, lty = 1:2)
```


---
# Traits 
Not discussed in lecture. See here for introduction:  
https://rpubs.com/dwarton/68823

---
class: inverse, center, middle 

# Vector Genralized Models 
---
# VGLMs
.footnote[see Dobson & Barnett (2018) for more on the exponential family]

... are an extension of GLMs   
GLMs are restricted to distributions from the .blue[exponential family].   
VGLMS are not.  
VGLMs can have multiple linear predictors, for different parameters.  
Explanatroy variables can differ between predictors.
---
# The exponential family

$$f(x, \theta) = s(x)\ t(\theta)\ exp (a(y)\ b(\theta))$$
--
$$f(x, \theta) = exp(a(y)\ b(\theta) + c(\theta) + d(y)) $$
with $c(\theta) = exp(t(\theta))$ and $d(y) = exp(s(y))$

--

.blue[Poisson]
\begin{align}
f(x,\lambda) &= \frac{\lambda^y e^{-\lambda}}{y!}\\
\theta &= \lambda\\
f(x, \theta) &= exp(log(\theta)\ y - \theta - log(y!))
\end{align}


---
# Reduced Rank VGLM  

Reduce the dimensions, i.e. number of predictors $P$ by reduced rank regression   

--

Create latent variables $\nu$ from subset of environmental variables $X$

--

$x = (x_1, x_2)$ and $B = c(B_1,B_2)$  

--
$$B_2 = A C^T$$ 
$$\nu = C^T x_2\ \ \ \ \ \ (site\ scores)$$
$$\eta = B_1^T x_1 + A C^Tx_2 = B_1^T x_1 + A \nu$$
--
$\rightarrow$ Reduced Rank Regression is the same as RDA! 


---

# Quadratic RR-VGLM

In ecology, unimodal responses are the norm (Oksanen & Minchin, 2002)  

--

quadratic linear predictor  

--

$$\eta = B_1^T x_1 + A C^Tx_2 = B_1^T x_1 + A \nu\ \ \ \ \ \ \ ordinary\ RR-VGLM$$
$$\eta_s = \beta_{1(s)}^T  + \beta_{2(s)}\ \nu + \beta_{3(s)}\ \nu^2\ \ \ \ \ \ \ quadratic\ RR-VGLM$$
$$ log\ \mu_s(\nu) = \eta_s = \alpha_s - \frac{1}{2} \bigg( \frac{\nu - u_s}{t_s} \bigg)^2 $$


---
# Hunting Spider Data

from ter Braak (1986)  

```{r load VGAM, message=FALSE, echo = F}
library(VGAM);library("DT"); library("dplyr");library(kableExtra)
data(hspider)
```

```{r, echo = F}

hspider %>% 
  kbl(format = "html") %>% 
  scroll_box(height = "400px", width = "700px")

```
---
# Rank-1 Poisson CQO
.scroll-output[
```{r run cqo, message=FALSE, warning=F}
set.seed(1234)
hspider[, 1:6] <- scale(hspider[, 1:6]) # Standardized environmental variables
p1ut.hs <- cqo(cbind(Alopacce, Alopcune, Alopfabr, 
                     Arctlute, Arctperi, Auloalbi, 
                     Pardlugu, Pardmont, Pardnigr, 
                     Pardpull, Trocterr, Zoraspin) ~ 
                 WaterCon + BareSand + FallTwig + CoveMoss + 
                 CoveHerb + ReflLux,
               family = poissonff, 
               data = hspider, 
               eq.toler = FALSE, 
               trace = FALSE)
```
]

```{r options cqo plot1, echo =F}
S <- ncol(depvar(p1ut.hs)) # Number of species
clr <- (1:(S+1))[-7] # Omits yellow
```
---
# Latent Variable Plot 
```{r cqo lvplot 1, message=FALSE, warning=F, fig.align="center"}
lvplot(p1ut.hs, y = TRUE, lcol = clr, pch = 1:S, pcol = clr)
```

---
# 2d Perspective Plot 
```{r CQO persp plot, fig.align="center"}
persp(p1ut.hs, col = clr, label = TRUE) # Perspective plot
```

---
# Constrained Coefficients
```{r 1d CQO variables}
concoef(p1ut.hs) %>%  as.vector() %>% round(2) -> p1ut_coef
names(p1ut_coef) <- row.names(concoef(p1ut.hs))
print(p1ut_coef)
```

--
.scroll-box-8[
```{r 1d CQO Tolerances}
Tol(p1ut.hs)[1, 1, ]
```
]
---
# Let's add a second gradient
.small[
```{r run 2d CQO, echo=T, results='hide'}
p2et.hs <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,Auloalbi, Pardlugu, Pardmont, Pardnigr, ... = Pardpull,Trocterr, Zoraspin)
               ~ WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
               poissonff, data = hspider, Crow1positive = FALSE, Rank = 2,
               I.toler = TRUE, Bestof = 2)
```
]
---
# 2 Graident Latent Variable Plot 

```{r, 2d latent variable plot, fig.align="center" , echo = F}
lvplot(p2et.hs, ellipse = FALSE, label = TRUE, xlim = c(-3, 5.7),
C = TRUE, Ccol = "brown", sites = TRUE, scol = "gray50",
pcol = "blue", pch = "+", chull = TRUE, ccol = "gray50")
```

---
# 3d Perspective plot
```{r CQO 2d persp plot, echo=T, results='hide', fig.align="center"}
persp(p2et.hs, xlim = c(-6, 5), ylim = c(-6, 3), 
      theta = 120, phi = 20)
```
---
# Extended ouput 

.scroll-output[
```{r CQO coefficients}
coef(p2et.hs)
```

]
---
# Constained Additive Ordination 
Besides VGLMs, additive models VGAMs can be fitted.  
- Syntax similar to CQO 
- new argument: *df1.nl*: effective non-linear degrees of freedom  
- controls how smooth the output is (0 = linear)
.scroll-output[
```{r fit CAO}
set.seed(1)
# p1cao.hs <- cao(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi, Auloalbi,
#                       Pardlugu, Pardmont, Pardnigr, Pardpull, Trocterr, Zoraspin) ~
#                 WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
#                 poissonff, data = hspider, Rank = 1, df1.nl = 2, Bestof = 10, 
#                 Crow1positive = FALSE)
```

]
---
# CAO single species plots

```{r, cao plot, fig.align="center", echo = F}
# par(mfrow = c(3,3))
# plot(p1cao.hs, lcol = "blue", lwd = 2, ylim = c(-5, 5), xlab = "", ylab = "")
```
---
```{r, cao persp, fig.align="center", echo = F}
# par(mfrow = c(1,1))
# persp(p1cao.hs,col = clr, label = TRUE)
```
---
# In practice 

- They are not commonly used 
- individual GLMs outperformed CQO in predicting species ranges (Baselga & Araújo, 2009)
- CQO better for rare species but GLM predicts better (Bonthoux *et al.* 2013) 
- mvabund glms differentiated between noise and causal variables better than CQO (Jupke & Schäfer, 2020)
- CQO and CAO predict better (but still bad) than GLM and GAM to dissimilar conditions (Maguire *et. al.*, 2016) 
- Applied studies mostly with fish (e.g. Vilizzi et al. (2012), Top *et al.* (2016))
- "limitations on the number of species, steepness of the learning curve and low numerical stability of the algorithm" (ter Braak & Šmilauer 2015)
---
class: inverse, center, middle
# Latent Variable Models 
---

# Think back to GLMM 

\begin{align}
g(m_{ij}) &= \alpha_i + \beta_{0i} + x_j^T \beta_i + u_{ij}\\
y_{ij}|u_{ij} &\sim F(m_{ij}, \phi)\\
u_i & \sim N(0, \Sigma)
\end{align}

--

--- 
\begin{align}
g(m_{ij}) &= \alpha_i + \beta_{0i} + x_j^T \beta_i + u_{ij}\\
u_i &= z_i^T \lambda_j\\
y_{ij}|z_{i} &\sim F(m_{ij}, \phi)\\
z_{i} &\sim N(0, I)
\end{align}

--
$\rightarrow$ basically an ordination of the covariance matrix
---
```{r}
library(gllvm);library(corrplot);library(gclus)
data("antTraits")
y <- as.matrix(antTraits$abund)
X <- scale(as.matrix(antTraits$env))
TR <- antTraits$traits
```
---
# Unconstrained ordination with gllvm
.scroll-output[
```{r}
fit_uo_po <- gllvm(y, family = poisson())
fit_uo_nb <- gllvm(y, family = "negative.binomial")
fit_uo_po
fit_uo_nb
par(mfrow = c(1,2))
plot(fit_uo_nb, which = 1)
plot(fit_uo_po, which = 1)
plot(fit_uo_nb, which = 2)
plot(fit_uo_po, which = 2)
plot(fit_uo_nb, which = 3)
plot(fit_uo_po, which = 3)
```
]
---
# Biplot with gllvm 

```{r gllvm unconstrained biplot, fig.align = "center", fig.height=7, echo = F}
ordiplot(fit_uo_nb, biplot = TRUE, ind.spp = 15, xlim = c(-3,3) , ylim = c(-2, 1.6))
```
---
# Constrained Ordination with gllvm 
```{r}
fit_co_nb2 <- gllvm(y, X, num.lv = 2, 
                    formula = ~ Bare.ground + Shrub.cover + Volume.lying.CWD, 
                    family = "negative.binomial")
fit_co_nb3 <- gllvm(y, X, num.lv = 3, 
                    formula = ~ Bare.ground + Shrub.cover + Volume.lying.CWD, 
                    family = "negative.binomial")
```
---
# Biplot of constrained ordination 
```{r gllvm constrained biplot, echo = F, fig.align ="center"}
ordiplot(fit_co_nb3, biplot = T)
```
---
# Coefficient plot 
```{r gllvm constrained coefficient plot, echo = F, fig.align = "center", fig.width = 10}
coefplot(fit_co_nb3, cex.ylab = 0.7, mar = c(4,9,2,1), 
         xlim.list = list(NULL, NULL, c(-4,4)))

```
---
# Residual Correlation plot 
```{r gllvm Residual Correlation plot, echo = F, fig.align="center"}
cr <- getResidualCor(fit_co_nb3)
par(mfrow = c(1,1))
corrplot(cr[order.single(cr), order.single(cr)], diag = FALSE, type = "lower", method = "square", tl.cex = 0.8, tl.srt = 45, tl.col = "red")
```
---
# Fitting a Fourth Corner Model
```{r}
fit_4th <- gllvm(y, X, TR, 
                 family = "negative.binomial", 
                 num.lv = 2, 
                 formula = y ~ 
                   (Bare.ground + Shrub.cover + Volume.lying.CWD) +
                   (Bare.ground + Shrub.cover + Volume.lying.CWD) :  
                   (Pilosity + Polymorphism + Webers.length))

```
---
# Fourth Corner Interaction Plot
```{r, echo = F, fig.align = "center"}
fourth <- fit_4th$fourth.corner
colort <- colorRampPalette(c("blue", "white", "red"))
a <- max( abs(fourth) )
plot.4th <- lattice::levelplot((as.matrix(fourth)), xlab = "Environmental Variables", 
                      ylab = "Species traits", col.regions = colort(100), cex.lab =1.3,
                      at = seq(-a, a, length = 100), scales = list(x = list(rot = 45)))
plot.4th
```
---
# Do Traits matter here? 
```{r}
fit_4th2 <- gllvm(y, X, TR, family = "negative.binomial", num.lv = 2, 
                  formula = y ~ (Bare.ground + Shrub.cover + Volume.lying.CWD))
anova(fit_4th, fit_4th2)
```
---

class: inverse, center, middle 


# Copulas 

---
class: inverse, center, middle 

# Hierarchical modeling of species communities 

---
class: inverse, center, middle 

# Hierarchical Neural networks 

--- 



