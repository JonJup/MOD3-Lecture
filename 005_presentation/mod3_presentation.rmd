---
title: "Advances in the analysis of multivariate ecological data"
subtitle: "MOD3: Advanced data science"
author: "Jonathan Jupke"
institute: "University of Koblenz-Landau"
date: "2021/01/07"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "libs/sydney.css", "libs/sydney-fonts.css"]
    nature:
      countIncrementalSlides: false
      navigation:
        scroll: false # disable slide transitions by scrolling

---
```{css, echo = FALSE}
.remark-code { font-family: 'Source Code Pro', 'Lucida Console', Monaco, monospace;
                                    font-size: 100%;
                                  }
}
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = FALSE,fig.pos = 'H',fig_cap=TRUE)
pacman::p_load(corrplot, 
               data.table,
               dplyr, 
               here,
               Hmsc,
               ggplot2,
               ggtree,
               kableExtra, 
               magrittr,
               tmap, 
               sf,
               UncertainOrd)
tmap_mode("view")
```

```{r todo list}
#TODO Abstand zwischen Bullets
#TODO Abstand zwischen Bullets
#TODO serial slide
#TODO adjust images 
#TODO Latent variable intro slide 

```
For these slides and additional material see [here](https://jonjup.netlify.app/post/)
---

# Plan for today 

1. What’s new in multivariate analysis?

2. mvabund

3. Vector GLM/GAM

4. Latent Variable Models 

5. Hierarchical Modeling of Species Communities

6. Copulas 

7. Neural Hierarchical Models 


---
class: inverse, center, middle 

# What’s new in multivariate analysis?

---

# ... well first, what's old? 

--
.pull-left[
  .blockquote[
    ### `r icon::fa("comment-dots")` Question
    What does multivariate mean? 
  ]
]

.pull-right[
  .blockquote[
    ### `r icon::fa("comment-dots")` Question
    What methods do you know?
  ]
]

--

.pull-left[
- multi-response   
- **Y** is a matrix not a vector
]

--

.pull-right[
- PCA  
- RDA  
- PERMANOVA  
- CA/ CCA  
- Random Forests   
- Clustering Methods   
]


---

# What does it look like in R? 

```{r load vegan, results='hide', echo = T, message = F}
# library(vegan)
# data(dune)
# data(dune.env)
# rda_object <- rda(dune ~ ., dune.env)
```

---
# Bi/Triplot
```{r wn-prepare-biplot, echo=FALSE}
# par(mar=c(0,0,0,0))
```

```{r wn-biplot, fig.width = 7, fig.height = 7, fig.align="center"}
# plot(rda_object)
```

---
# Common ground? 

--

They are based on some notion of a .blue[**distance metric**].  

--

.pull-left[
.blue[**fixed**]

- PCA and RDA: Euclidean 
- CA and CCA: $\chi^2$-distance
]

--

.pull-right[
.blue[**flexible**]

- NMDS 
- PCoA
- dbRDA
]

--

.blockquote[
  ### `r icon::fa("comment-dots")` Question
  How does this differ form univariate analysis?
]
---



# Models! 

In univariate analyses we mostly use models:

--

- linear model 

--

- linear mixed model 

--

- generalized linear model 

--

- generalized additive models 

--

`r icon::fa("question-circle")` why not in multivariate analyses?

---
class: middle, center

```{r, fig.align="center", echo = F, fig.width=8, fig.height=8}
# knitr::include_graphics("old_pc.gif")
```

---
# Notation 

**Y** is a N $\times$ S matrix of occurrences\ abundances with elements $y_{is}$  
**X** is a N $\times$ P matrix of environmental variables with elements $x_{ip}$   
$\mathbf{\Lambda}$ is a R $\times$ S matrix of species loadings with elements $\lambda_{rs}$   
$\mathbf{\Psi}$ is a N $\times$ R matrix of site loadings with elements $\psi_{ir}$    
**T** is a S $\times$ J matrix of species traits with elements $t_{sj}$   
---
.footnote[Gauch & Whittaker (1972), Zuur et al. (2007)]
# The idea is old 

The Gaussian response model: 
$$Y_{si} = c_s\ exp\bigg(- \frac{(X_i - u_s)^2}{2t_s^2}\bigg) $$
--

.blue[c]: maximal abundance, .blue[u]: location of optima, .blue[t]: tolerance  

--

As GLM:
\begin{align}
Y_{si} &= exp \bigg(ln(c_s) - \frac{u_s^2}{2t_s^2} + \frac{u_s}{t_s^2}x_i - \frac{1}{2t_s^2}x^2_i\bigg)
&=exp(b_{1s} + b_{2s}\ x_i + b_{3s}\ x_i^2)
\end{align}

--

with  

$$t_s = \frac{1}{\sqrt{-2b_{3s}}}; u_s = \frac{-b_{2s}}{2b_{3s}}; c_s = exp \bigg(b_{1s} - \frac{b_{2s}^2}{4b_{3s}}\bigg)$$ 

---
# Restricted Gaussian Regression 

Uses $(1+2P)S$ parameters.  
For 10 species and 5 variables 110 parameters.   

--

Gradients $\mathbf{\Psi}$ as linear combinations of measured variables $x$:
$$\psi_r = \Sigma_{p =  1}^P \alpha_{rp}\ x_p$$
--

Plug gradients back into Gaussian regression
$$Y_{is} = c_s\ exp\bigg(\frac{(\psi_{ir} - u_{sr})^2}{2t_{sr}^2}\bigg)$$

If R $\ll$ P, this reduces the number of parameters. 

---
# What is the model-based approach? 

Model-based approaches assume a parametric generative model

--

.blue[**Terms are not consistent**]:  
Roberts (2019) and Warton *el al.* (2012) disagree about  CA

--

Alternative 1: algorithm-based (Warton *et al.* 2015)  

--


Alternative 2: transformation-based  

--

<blockquote>
Our constrained ordination model [...] 
.right[-- <cite>Brittain *et al.* (2020)</cite>]
</blockquote>

--

<blockquote>
Many biologist fit CCA models [...] 
.right[-- <cite>Yee (2004) </cite>]
</blockquote>


---
background-image: url("figures/glm_error.png")
background-size: 550px
background-position: 50% 70%

# Are models better? 

---
background-image: url("figures/WWW12_1.png")
background-size: 700px
background-position: 50% 50%

# Are models better? 

Every distance metric assumes a mean-variance relationship  

.footnote[Warton *et al.* (2012)]
---
background-image: url("figures/WWW12_2.png")
background-size: 500px
background-position: 50% 75%

# Are models better? 

Every distance metric assumes a mean-variance relationship  

.footnote[Warton *et al.* (2012)]
---
background-image: url("figures/WWW12_3.png")
background-size: 500px
background-position: 50% 75%

# Are models better? 

Every distance metric assumes a mean-variance relationship  

.footnote[Warton *et al.* (2012)]
---
background-image: url("figures/szöcs15_1.png")
background-size: 800px
background-position: 50% 50%

# Are models better? 

.footnote[Szöcs *et al.* (2015)]
---
background-image: url("figures/jupke20_1.png")
background-size: 800px
background-position: 50% 50%

# Are models better? 

.footnote[Jupke *et al.* (2020)]
---
class: inverse, center, middle 

# mvabund

---
# mvabund 

R package published in 2012  (Wang et al. 2012)

**M**ulti**v**ariate **Abund**ance data  

--

Multivariate
- many, possibly correlated responses 

--

Abundance
- strong mean-variance relationship 
---
# Multivariate GLMM

We could use mixed models and model the taxon as a random effect.

--

$$Abundance \sim species\ + variable1\ + variable2\ + (0 + species|sample)$$
--

1. $y_{is}$ are .blue[independent], conditional on mean  $\mu_{is}$

--

2. Conditional on $\mu_{is}$, data come from a .blue[known distribution]  

--

3. .blue[straight line relationship] between some function $\mu_{is}$ and x, with error $\epsilon_{is}$

$$g(\mu_{is}) = \beta_{0s} + x^T_i \beta_s + \epsilon_{is}$$
--

4. $\epsilon$ are normally distributed. 
$$\epsilon_{is} \sim MVN(0, \Sigma)$$
--

They introduce correlation between response within observations. 

---
# Implementations in R

- lme4 (Bates *et al*., 2015)
- MCMCglmm (Hadfield, 2010)
- purpose written code (e.g. Pollock *et al*. 2014)

--

.blockquote[
  ### `r icon::fa("comment-dots")` Question
  What is problematic with this approach?
]

---
# Problem? Parameters! 

|  # responses | # parameters in $\Sigma$ |
| :---- |:--- |
| 5     | 15 |
| 10    | 55 |
| 20    | 210 |
| 40    |  820 |

--
.pull-left[
.blue[maximum likelihood]  
- No convergence
]

--

.pull-right[
.blue[bayesian]  
- long run time  
- not all parameters converge  
- big influence of prior
]

---
# Simplify

Instead of estimating we can assume $\Sigma$.

--

mvabund uses design-based inference.  

--

Ok, if primary interest is species environment relationships  

--
Multivariate: row resampling for inference, preserves species correlations  
Abundance: separate GLM for each species
---
# Test statistics  

.content-box-blue[Sum of Likelihood Ratios]  
\begin{align}
L & = 2 \times (l_M - l_m)  \\
L &\sim \chi^2_{p_M - p_m} \\  
\end{align}

--

Test statistic does not account for correlations but the *p* -value.  

--
   
.content-box-blue[Wald statistic]  
\begin{align}
W &= \frac{(\beta_m - \beta_0)^2}{var(\beta_m)} \\
z &= \sqrt(W)
\end{align}

--

.content-box-blue[Score statistic]   
see e.g. Dunn & Smyth (2018)

---
# Correlation options
  
.content-box-blue["I"]  
No correlation, default setting  

--

.content-box-blue["R"]   
estimates $\Sigma$, see mixed model 

--

.content-box-blue["shrink"]  
middle option, shrinks correlation matrix to $I$ with ridge regularization (Warton, 2008)  
  
--
  
For .blue[R] and .blue[shrink] $\Sigma LR$ is not available. They are estimated with generalized estimation equations (Warton, 2011) which do not produce likelihoods. 

---

```{r}
# library(mvabund)
# data("Tasmania")
# attach(Tasmania)
# head(copepods[,1:3])
```

--

```{r}
# tasmvabund <- mvabund(copepods)
# class(tasmvabund)
```

---

```{r, message = FALSE, fig.align="center", warning=FALSE, fig.height=6}
# plot(tasmvabund~treatment, col = as.numeric(block))
```
---
```{r, message = FALSE, warning=FALSE, fig.align="center",  fig.height=7}
# meanvar.plot(copepods~tr.block, col = as.numeric (treatment))
```
---
# Fitting a model 
```{r}
# tas.nb <- manyglm(copepods ~ block*treatment, 
#                   family = "negative.binomial")
```


$$Y_{is} \sim NB(\mu_{skl}, \phi_s)$$
$$log(\mu_{jkl}) = intercept_s + block_{skl} + treatment_{sl} + block \times treatment_{skl}$$
with treatment level .blue[k] and block .blue[l]

---
# Checking model assumptions 
```{r mvabund Checking model assumptions 1, fig.align="center", fig.height=6, fig.width=6}
# plot.manyglm(tas.nb, which = 1)
```

.footnote[Dunn & Smyth (1996)]

---

# Checking model assumptions
```{r mvabund Checking model assumptions 2, fig.align="center", fig.height=6, fig.width=6}
# plot.manyglm(tas.nb, which = 2)
```

.footnote[Dunn & Smyth (1996)]

---

# Checking model assumptions
```{r mvabund Checking model assumptions 3, fig.align="center", fig.height=6, fig.width=6}
# plot.manyglm(tas.nb, which = 3)
```

.footnote[Dunn & Smyth (1996)]

---

# Testing Hypotheses

```{r mvabund Testing Hypotheses, comment=""}
# anova_out <- anova(tas.nb, p.uni = "adjusted")
```
.scroll-box-16[
```{r}
# anova_out
```
]

---

```{r mvabund-beta-plot, echo=FALSE, fig.align="center", fig.height=10, fig.width=10}
# library(lattice)
# a <- max(abs(coef(tas.nb)))
# colort <- colorRampPalette(c("blue", "white", "red"))
# plot.tas <- levelplot(t(as.matrix(coef(tas.nb))), ylab = "", xlab  = "", col.regions = colort(100), at=seq(-a,a,length = 100), scales = list( x= list(rot = 45)))
# print(plot.tas)
```

---

```{r mvabund-predictions, echo=FALSE, fig.align="center", fig.height=8, fig.width=8}
# tas_pred = predict(tas.nb, type = "response")
# matplot(t(tas_pred[c(1,3),]), type = "l", xaxt = "n", log = "y", ylab = "Mean abundance [log]")
# axis(1, at =1:12, labels = colnames(copepods), las = 3)
# legend("topright", legend = levels(treatment), col = 1:2, lty = 1:2)
```

---

# Traits 
Not discussed in lecture. See here for introduction:  
https://rpubs.com/dwarton/68823

---
class: inverse, center, middle 

# Vector Genralized Models 
---
# VGLMs
.footnote[see Dobson & Barnett (2018) for more on the exponential family]

... are an extension of GLMs 

--

- GLMs are restricted to distributions from the .blue[exponential family].   
VGLMS are not.  

--

- VGLMs can have .blue[multiple linear predictors], for different parameters.

--

- Explanatory variables can differ between predictors.

---

# Reduced Rank VGLM  

Reduce the dimensions, i.e. number of predictors $P$ by reduced rank regression   

--

Create latent variables $\psi$ from subset of environmental variables $x_2$

--

$x = (x_1, x_2)$ and $B = c(B_1,B_2)$  

--
$$B_2 = A C^T$$ 
$$\psi = C^T x_2\ \ \ \ \ \ (site\ scores)$$
--

\begin{align}
\eta &= B_1^T x_1 + A C^Tx_2 \\
     &= B_1^T x_1 + A \nu
\end{align}

--

`r icon::fa("exclamation-circle")` Reduced Rank Regression is the same as RDA! 


---

# Quadratic RR-VGLM

In ecology, unimodal responses are the norm (Oksanen & Minchin, 2002)  

--

\begin{align}
\eta &= B_1^T x_1 + A \psi\  & ordinary\ RR-VGLM \\
     &= \beta_{1(s)}^T  + \beta_{2(s)}\ \psi + \beta_{3(s)}\ \psi^2 & quadratic\ RR-VGLM
\end{align}

--

$$ log\ \mu_s(\psi) = \eta_s = \alpha_s - \frac{1}{2} \bigg( \frac{\psi - u_s}{t_s} \bigg)^2 $$


---
# Hunting Spider Data

from ter Braak (1986)  

```{r load VGAM, message=FALSE, echo = F}
# library(VGAM)
# data(hspider)
```

```{r, echo = F}
# hspider %>% 
#   kbl(format = "html") %>% 
#   scroll_box(height = "400px", width = "700px")
```
---
# Rank-1 Poisson CQO

```{r cqo-run-dont-show, message=FALSE, warning=FALSE, results='hide', echo=T}
# set.seed(1234)
# hspider[, 1:6] <- scale(hspider[, 1:6]) # Standardized environmental variables
# p1ut.hs <- cqo(cbind(Alopacce, Alopcune, Alopfabr, 
#                      Arctlute, Arctperi, Auloalbi, 
#                      Pardlugu, Pardmont, Pardnigr, 
#                      Pardpull, Trocterr, Zoraspin) ~ 
#                  WaterCon + BareSand + FallTwig + CoveMoss + 
#                  CoveHerb + ReflLux,
#                family = poissonff, 
#                data = hspider, 
#                eq.toler = FALSE, 
#                trace = FALSE)
```



```{r options cqo plot1, echo =F}
# S <- ncol(depvar(p1ut.hs)) # Number of species
# clr <- (1:(S+1))[-7] # Omits yellow
```
---
# Latent Variable Plot 
```{r cqo lvplot 1, message=FALSE, warning=F, fig.align="center", echo=FALSE}
# lvplot(p1ut.hs, y = TRUE, lcol = clr, pch = 1:S, pcol = clr)
```

---
# 2d Perspective Plot 
```{r CQO persp plot, fig.align="center", echo=F}
# persp(p1ut.hs, col = clr, label = TRUE) # Perspective plot
```

---
# Constrained Coefficients
```{r 1d CQO variables}
# concoef(p1ut.hs) %>%  as.vector() %>% round(2) -> p1ut_coef
# names(p1ut_coef) <- row.names(concoef(p1ut.hs))
# print(p1ut_coef)
```

--

```{r 1d CQO Tolerances}
# Tol(p1ut.hs)[1, 1, ]
```

---

# Let's add a second gradient

```{r run 2d CQO, echo=T, results='hide', message=FALSE,warning=FALSE}
# p2et.hs <- cqo(cbind(Alopacce, Alopcune, Alopfabr, 
#                      Arctlute, Arctperi, Auloalbi, 
#                      Pardlugu, Pardmont, Pardnigr,
#                      Pardpull,Trocterr, Zoraspin) ~
#                 WaterCon + BareSand + FallTwig + 
#                  CoveMoss + CoveHerb + ReflLux,
#                family = poissonff, 
#                data = hspider, 
#                Crow1positive = FALSE, 
#                Rank = 2,
#                I.toler = TRUE, 
#                Bestof = 2)
```

---

# Rank 2 Latent Variable Plot 

```{r, 2d latent variable plot, fig.align="center" , echo = F}
# lvplot(p2et.hs, ellipse = FALSE, label = TRUE, xlim = c(-3, 5.7),
# C = TRUE, Ccol = "brown", sites = TRUE, scol = "gray50",
# pcol = "blue", pch = "+", chull = TRUE, ccol = "gray50")
```

---
# 3d Perspective plot
```{r CQO 2d persp plot, echo=FALSE, results='hide', fig.align="center"}
# persp(p2et.hs, xlim = c(-6, 5), ylim = c(-6, 3), 
#       theta = 120, phi = 20)
```
---
# Extended ouput 

.scroll-output[
```{r CQO coefficients}
# coef(p2et.hs)
```

]
---
# Constained Additive Ordination 
In addition to VGLMs, additive models (VGAMs) can be fitted.  
- Syntax similar to CQO 
- new argument: *df1.nl* : effective non-linear degrees of freedom  
- controls how smooth the output is (0 = linear)
.small[
```{r fit CAO, message=FALSE, warning=FALSE,results='hide'}
# set.seed(1)
# p1cao.hs <- cao(cbind(Alopacce, Alopcune, Alopfabr, 
#                       Arctlute, Arctperi, Auloalbi,
#                       Pardlugu, Pardmont, Pardnigr, 
#                       Pardpull, Trocterr, Zoraspin) ~
#                 WaterCon + BareSand + FallTwig + 
#                   CoveMoss + CoveHerb + ReflLux,
#                 family = poissonff, 
#                 data = hspider, 
#                 Rank = 1, 
#                 df1.nl = 2,
#                 Bestof = 10,
#                 Crow1positive = FALSE)
```
]

---

# CAO single species plots

.scroll-output[
```{r, cao plot, fig.align="center", echo = F}
# plot(p1cao.hs, lcol = "blue", lwd = 2, ylim = c(-5, 5), xlab = "", ylab = "")
```
]

---

# CAO Latent variable Plot 
```{r, cao persp, fig.align="center", echo = F}
# par(mfrow = c(1,1))
# persp(p1cao.hs,col = clr, label = TRUE)
```
---
# In practice 

- They are not commonly used 

--

- GLM predicts species ranges better (Baselga & Araújo, 2009)

--

- CQO better for rare species (Bonthoux *et al.* 2013) 

--

- manyglms differentiated between noise and causal variables better than CQO (Jupke & Schäfer, 2020)

--

- CQO and CAO predict better (but still bad) than GLM and GAM to dissimilar conditions (Maguire *et. al.*, 2016) 

--

- Applied studies mostly with fish (e.g. Vilizzi et al. (2012))

--

<blockquote>
limitations on the number of species, steepness of the learning curve and low numerical stability of the algorithm
.right[-- <cite>ter Braak & Šmilauer 2015</cite>]
</blockquote>

---
class: inverse, center, middle
# Latent Variable Models 
---
# Latent variables ??? 

---
# Random-effects ordination 

.footnote[Walker & Jackson (2011)]

- unconstrained ordination through random effect model  

--

- $\hat{y_i} = a + \mathbf{B}\mathbf{x}_i$

--

- $a = [a_i]$ is a column vector of intercepts with one for each variable

--

- $B=[b_{jk}]$ matrix of coefficients relating observed variables to axes (latent variables)

--

- assume standard normal distribution of axis scores

--

- fit $\mathbf{B}$ and $x_i$ iteratively with ML 

--

- only presence-absence or normal distribution

--

- implemented in reo R package



---

# BORAL

- Hui *et al* (2015) extend the basic idea of Walker & Jackson (2011) to GLMs 

--

- Hui (2016): **B**ayesian **O**rdination and **R**egression **A**na**L**ysis

--

- (un)constrained Ordination with MCMC estimation 

--

- latent variables are biotic associations or missing covariates.

--

- $log(\mu_{ij}) = \alpha_i + \theta_{0j} + z_{i1} \theta_{1j} + z_{i2}\theta_{2j}=\alpha_i + \theta_{0j} +\mathbf{z_i^T}\mathbf{\theta_j}$

--

- Priors:
  - $\theta_{0j} \sim N(0,100)$
  - $\theta_{\lambda} \sim U(0,50)\ \forall\ \theta_{\lambda}:0\notin \theta_{\lambda}$ 
  - $\theta_j; \alpha_i \sim N(0,20)$
---

# Co-occurence is not Interaction 

.footnote[Blanchet *et al.* 2020]

- Species occurrences depend on the environment  

--

- Indirect species associations   

--

- Sampling scale influences measures of co-occurrence  

--

- Appropriate statistical inference requires a very large sample size  

--

- Interactions are often asymmetric   
---

# BORAL in Action

.scroll-output[
```{r read-boral-ucpo, echo=FALSE}
# fit_unconstrained_po <-readRDS("../003_processed_data/boral_unconstrained_poisson.RDS")
# pacman::p_load(boral, rjags, mvabund, UncertainOrd)
# data(spider)
# Y <- spider$abund
```

```{r boral-in-action, eval=F}
# pacman::p_load(boral, rjags, mvabund, UncertainOrd)
# data(spider)
# Y <- spider$abund
# fit_unconstrained_po <- boral(y = Y, 
#                               family = "poisson", 
#                               lv.control = list(num.lv = 2),
#                               row.eff = "fixed",
#                               save.model=TRUE)
# summary(fit_unconstrained_po)
```
```{r boral-in-action2, echo=F}
# summary(fit_unconstrained_po)
```

]

---

# Check model assumptions

```{r check-boral-ucpo, fig.align="center", fig.height=8, fig.width=8, echo=FALSE}
# par(mfrow=c(2,2))
# res <- capture.output(plot(fit_unconstrained_po))
```
---
# Check model assumptions - NB
```{r check boral 2, fig.align="center", fig.height=8, fig.width=8, echo=FALSE}
# fit_unconstrained_nb <-readRDS("../003_processed_data/boral_unconstrained_negbinom.RDS")
# par(mfrow=c(2,2))
# res = capture.output(plot(fit_unconstrained_nb))
```
---
# Ordination Diagram
```{r ordiplot boral 1, fig.align="center", fig.height=8, fig.width=8, echo=FALSE}
# suppressMessages(lvsplot(fit_unconstrained_nb))
```
---
```{r boral load constrained, echo=FALSE}
# fit_constrained_nb <- readRDS("../003_processed_data/boral_constrained_negbinom.RDS")
# X <- scale(spider$x)
```

# Constrained Ordination 

.scroll-output[
```{r boral fit constrained, eval = F}
# X <- scale(spider$x)
# 
# fit_constrained_nb <-
#   boral(
#     y = Y,
#     X = X,
#     family = "negative.binomial",
#     lv.control = list(num.lv = 2),
#     save.model = TRUE
#   )
# 
# summary(fit_constrained_nb)
```
```{r boral-fit-constrained-3,echo=F}
# summary(fit_constrained_nb)
```

]

---
# Correlation plots 
```{r boral correlation plots, echo=FALSE, fig.align="center", fig.width=8, fig.height=8}
# envcors <- get.enviro.cor(fit_constrained_nb)
# par(mfrow=c(1,1))
# corrplot(
#         envcors$sig.cor,
#         type = "lower",
#         diag = FALSE,
#         title =  "Correlations due to covariates", 
#         mar = c(3,0.5,2,1), 
#         tl.srt = 45) 
```
---
# Correlation plots 
```{r boral correlation plots2, echo=FALSE, fig.align="center", fig.width=8, fig.height=8}
# rescors <- get.residual.cor(fit_constrained_nb) 
# corrplot(
#         rescors$sig.cor,
#         type = "lower",
#         diag = FALSE,
#         title =  "Residual correlations",
#         mar = c(3, 0.5, 2, 1),
#         tl.srt = 45
# )
```
---
# Uncertainty in latent variable models
- R package UncertainOrd (Hoegh & Roberts, 2020)
- based on 95% credible intervals

```{r load uncertainord file, echo=FALSE}
# uncertain_plot <- readRDS("../003_processed_data/boral_uncertainplot.RDS")
# par(mfrow=c(1,1))
```
.scroll-output[
```{r, echo=FALSE}
# options(warn=-1)
# uncertain_plot[[1]]$plot.obj
# uncertain_plot[[2]]$plot.obj
# uncertain_plot[[3]]$plot.obj
# uncertain_plot[[4]]$plot.obj

```
]
---
# Can we include traits in this? 

- as parameters for the priors  

$$\theta_{0j} \sim N(t_j^T \kappa_0, \mathbf{\sigma}^2_0)$$
$$\beta_{jk} \sim N(\mathbf{t_j^T \kappa_K, \sigma^2_k})$$
- The mean of the normal distribution from which species-specific intercept and regression coefficients are drawn depends on traits (t) and a trait coefficient ($\kappa$). 
---
# GLLVM package 

- R package gllvm (Niku *et al.*, 2019b)

- Maximum Likelihood instead of Bayesian 

- faster! 

---
```{r gllvm-in-action-1}
# pacman::p_load(gllvm, corrplot,gclus)
# data("antTraits")
# y <- as.matrix(antTraits$abund)
# X <- scale(as.matrix(antTraits$env))
# TR <- antTraits$traits
# fit_uo_po <- gllvm(y, family = poisson())
# fit_uo_nb <- gllvm(y, family = "negative.binomial")
# fit_uo_po
```
---
# Unconstrained ordination
.scroll-output[
```{r gllvm-in-action-2, echo=FALSE, fig.width=8, fig.hight=8, fig.align="center"}
# par(mfrow = c(1,2))
# plot(fit_uo_nb, which = 1)
# plot(fit_uo_po, which = 1)
# plot(fit_uo_nb, which = 2)
# plot(fit_uo_po, which = 2)
# plot(fit_uo_nb, which = 3)
# plot(fit_uo_po, which = 3)
```
]
---
# gllvm Biplot  
```{r gllvm-uc-biplot, fig.align = "center", fig.height=8, fig.width=8, echo = F}
# ordiplot(fit_uo_nb, 
#          biplot = TRUE, 
#          ind.spp = 15, 
#          xlim = c(-3,3) , 
#          ylim = c(-2, 1.6))
```
---
# Constrained Ordination 
```{r gllvm-in-action-3}
# fit_co_nb2 <- gllvm(y, X, num.lv = 2,
#                     formula = ~ Bare.ground + Shrub.cover + 
#                       Volume.lying.CWD,
#                     family = "negative.binomial")
# fit_co_nb3 <- gllvm(y, X, num.lv = 3,
#                     formula = ~ Bare.ground + Shrub.cover + 
#                       Volume.lying.CWD,
#                     family = "negative.binomial")
# AIC(fit_co_nb2, fit_co_nb3)
```
---
# Biplot of constrained ordination 
```{r gllvm-co-biplot, echo = F, fig.align ="center", fig.height=8, fig.width=8}
# ordiplot(fit_co_nb2, biplot = T)
```
---
# Coefficient plot 
```{r gllvm-co-coefficient-plot, echo = F, fig.align = "center", fig.width = 10, fig.height=7}
# coefplot(fit_co_nb2, cex.ylab = 0.7, mar = c(4,9,2,1),
#          xlim.list = list(NULL, NULL, c(-4,4)))
```
---
# Residual Correlation plot 
```{r gllvm-residual-correlation-plot, echo = F, fig.align="center",fig.height=8, fig.width=8}
# par(mfrow = c(1,1))
# cr <- getResidualCor(fit_co_nb2)
# corrplot(cr[order.single(cr), 
#             order.single(cr)], 
#          diag = FALSE, 
#          type = "lower", 
#          method = "square", 
#          tl.cex = 0.8, 
#          tl.srt = 45, 
#          tl.col = "red")
```
---
# Fitting a Fourth Corner Model
```{r fitting-4th-corner-gllvm}
# fit_4th <- gllvm(
#   y = y,
#   X = X,
#   TR = TR,
#   family = "negative.binomial",
#   num.lv = 2,
#   formula = y ~
#     (Bare.ground + Shrub.cover + Volume.lying.CWD) +
#     (Bare.ground + Shrub.cover + Volume.lying.CWD):(Pilosity + Polymorphism + Webers.length)
# )

```
---
# Fourth Corner Interaction Plot
```{r, echo = F, fig.align = "center",fig.height=8, fig.width=8}
# fourth <- fit_4th$fourth.corner
# colort <- colorRampPalette(c("blue", "white", "red"))
# a <- max( abs(fourth) )
# plot.4th <- lattice::levelplot((as.matrix(fourth)), xlab = "Environmental Variables",
#                       ylab = "Species traits", col.regions = colort(100), cex.lab =1.3,
#                       at = seq(-a, a, length = 100), scales = list(x = list(rot = 45)))
# plot.4th
```
---
# Do Traits matter here? 
```{r gllvm-traits-matter}
# fit_4th2 <- gllvm(y, X, TR, family = "negative.binomial", num.lv = 2,
#                   formula = y ~ (Bare.ground + Shrub.cover + Volume.lying.CWD))
# anova(fit_4th, fit_4th2)
```
---

Berts stuff 

---
class: inverse, center, middle 

# Hierarchical modeling of species communities 
---
background-image: url("figures/Elith2007_1.png")
background-size: 900px
background-position: 50% 50%
# Species Distribution Models
.footnote[Elith *et al.* (2009)]
---
# Species Distribution Models

- aka niche models $^1$, environmental models $^2$, habitat suitability models $^2$, envelope models $^3$
- model the realized niche 
.footnote[1. Peterson *et al.* (2011), 2. Guisan *et al* (2017), 3. Hijmans & Graham (2006)]
---
background-image: url("figures/zurell2020-1.png")
background-size: 800px
background-position: 50% 50%
# Species Distribution Models 
.footnote[Zurell *et al.* (2020)]
---
# Joint Species Distribution Models

- biotic interactio   ns are also important! (Wisz *et al.*, 2013)

--

- limit range to that of another species (Schweiger *et al.*, 2012)

--

- use species at predictor variables (Leathwick & Austin, 2001)

--

- JSDMs use resdiual correlation latent variables (Pollock *et al.* 2014)

--

\begin{align}
Pr(Y_{ij}=1) & = Pr(Z_{ij} > 0) \\
Z_{ij} & \sim N(X_iB, \Sigma) \\
B_{jk} & \sim N(\mu_k, \sigma_k)\\
\end{align}

---

# Are JSDMs worth the trouble? 

- alternative is called stacked SDM (many single species models)

--

- many studies compare predictive performance 

--

- JSDMs better for communities with many rare species but rank order differs according to evaluation metric (Norberg *et al.*, 2019)

--

- no difference in performance (Zurell *et al.*, 2019)

--

- community level models  "predict first, assemble later" vs. "assemble first, predict later" 

--

- clm outperform stacked models slightly (Maguire *et al.*, 2016)

--

- clm perform poorly (Baselga & Araújo, 2010)

---
background-image: url("figures/ovaskainen2017_1.png")
background-size: 550px
background-position: 50% 75%
# Hierarchical modeling of species communities
.footnote[Ovaskainen *et al.* (2017)]

---
background-image: url("figures/ovaskainen_abrego_2020-1.png")
background-size: 550px
background-position: 50% 70%
# Hierarchical modeling of species communities
.footnote[Ovaskainen & Abrego (2020)]

---
# HMSC - Single Species Example

```{r hmsc-ss-load ,echo=F, results='hide'}
# library(Hmsc)
# data.directory="../001_raw_data/hmsc_birds/data/"
# model.directory="../003_processed_data/hmsc_mcmc/"
# data = fread(file.path(data.directory, "data.csv"), stringsAsFactors=TRUE)
# data=data[Year==2014]
# data%<>%droplevels()
# #XData=as.data.frame(data[,c("Habitat", "AprMay")])
# #names(XData)=c("hab","clim")
# #Y=as.matrix(data$Corvus_monedula)
# #colnames(Y)="Corvus monedula"
# xy=as.matrix(data[,c("x","y")])
# studyDesign=data.frame(route=factor(data$Route))
# rownames(xy)=studyDesign[,1]
# #rL=HmscRandomLevel(sData = xy)
# #XFormula=~hab+poly(clim, degree=2, raw=TRUE)
```

```{r hmsc-ss-show-data, echo=F}
# data %>% 
#   head() %>% 
#   kable(format = "html") %>% 
#   kable_minimal() %>% 
#   scroll_box(height = "300px", width = "800px")
```
---
# HMSC - Single Species Example
```{r hmsc-ss-define-full-model}
# m_full = Hmsc(Y=as.matrix(data$Corvus_monedula),
#               XData=as.data.frame(data[,c("Habitat", "AprMay")]),
#               XFormula=~Habitat+poly(AprMay, degree=2, raw=TRUE),
#               distr = "lognormal poisson",
#               studyDesign = data.frame(route=factor(data$Route)),
#               ranLevels=list(route=HmscRandomLevel(sData = xy)))
```

--

.blue[**lognormal poisson?**]

--

- instead of negative binomial
- Poisson: $y_i \sim Poisson(exp(L_i))$
- lognormal Poisson: $y_i \sim Poisson(exp(L_i + \epsilon_i)), \ \epsilon \sim N(0, \sigma^2)$ 

---
# Fit model with MCMC
```{r fit-ss-hmsc, eval=F}
# nChains = 2
# thin=c(5,10,100)
# nParallel = max(round(parallel::detectCores()/2), nChains)
# samples = 1000
# transient=500*thin
# verbose=500*thin
# 
# for (i in 1:3) {
#         models[[i]] = sampleMcmc(
#                 models[[i]],
#                 thin = thin[i],
#                 samples = samples,
#                 transient = transient,
#                 nChains = nChains,
#                 verbose = verbose,
#                 initPar = "fixed effects"
#         )
# }
```

---
```{r hmsc-load-ss-model, echo=F}
# files = fs::dir_ls(file.path(model.directory, "corvusmonedula"))
# name_id = c(10,100,5)
# for (i in seq_along(files)) {
#   assign(x = paste0("models", name_id[i]),
#          value = readRDS(
#            files[i]
#          ))
# }
# rm(files, name_id)
# mpost1 = convertToCodaObject(
#         models5[[1]], 
#         spNamesNumbers = c(T,F), 
#         covNamesNumbers = c(T,F)
#         )
# mpost2 = convertToCodaObject(
#         models10[[1]], 
#         spNamesNumbers = c(T,F), 
#         covNamesNumbers = c(T,F)
#         )
# mpost3 = convertToCodaObject(
#         models100[[1]], 
#         spNamesNumbers = c(T,F), 
#         covNamesNumbers = c(T,F)
#         )
```
# thinning=5
.scroll-output[
```{r hmsc-ss-beta-plot1, fig.align="center", fig.height=9, fig.width=9, echo=F}
# plot(mpost1$Beta)
```
]
---
# thinning=10
.scroll-output[
```{r hmsc-ss-beta-plot2, fig.align="center", fig.height=9, fig.width=9,echo=F}
# plot(mpost2$Beta)
```
]
---
# thinning=100
.scroll-output[
```{r hmsc-ss-beta-plot3, fig.align="center", fig.height=9, fig.width=9,echo=F}
# plot(mpost3$Beta)
```
]
---
# Other MCMC convergence metrics
.blue[**Effective sample size**]
```{r hmsc-ss-effective-sample-size, echo=F}
# rm(mpost1, mpost2)
# ess.beta = effectiveSize(mpost3$Beta)
# temp = data.frame (ess=ess.beta)
# vc_names_ess = names(ess.beta)
# vc_names_ess %<>% 
#   stringr::str_remove("^B\\[") %>% 
#   stringr::str_remove(", Corvus monedula\\]")
# rownames(temp) = vc_names_ess
# temp %>% 
#   kable(format="html") %>% 
#   kable_minimal %>% 
#   scroll_box(height = "200px", width = "800px")
# rm(ess.beta, temp, vc_names_ess)
```

--

.blue[**potential scale reduction factor**] 

```{r hsmsc--ss-psrf, echo=F}
# psrf.beta = gelman.diag(mpost3$Beta,
#                         multivariate = FALSE)$psrf
# vc_names_psrf = rownames(psrf.beta)
# vc_names_psrf %<>% 
#   stringr::str_remove("^B\\[") %>% 
#   stringr::str_remove(", Corvus monedula\\]")
# rownames(psrf.beta) = vc_names_psrf
# psrf.beta %>% 
#   as.data.frame %>% 
#   kable(format="html") %>% 
#   kable_minimal %>% 
#   scroll_box(height="200px", width="800px")
# rm(psrf.beta, vc_names_psrf)
```
---
# Cross-Validation Performance 
```{r hmsc-ss-predictive-performance, echo = F}

# preds=computePredictedValues(models100[[1]], expected = FALSE)
# MF=evaluateModelFit(hM=models100[[1]], predY=preds)
# 
# MF %>% 
#   unlist %>% 
#   as.data.frame() %>% 
#   kable(format = "html") %>% 
#   kable_minimal() %>% 
#   scroll_box(height="300px",width="800px")
```

--

.blue[**O**] - Occurrence, .blue[**C**] - Count

--

.blue[**SR**]: pseudo- $R^2$ based on squared spearman correlation  

--

.blue[**AUC**]: Area under Curve - higher = better   

--

.blue[**TjurR2**]: $\bar{\hat{y_i}}- \bar{\hat{y_j}}, \forall y_i = 1 \land  y_j = 0$
---
# Variance Partioining
```{r hmsc-ss-design, echo=F}
# groupnames = c("habitat", "climate")
# group=c(1,1,1,1,1,2,2)
# VP = computeVariancePartitioning(models100[[1]], 
#                                  group=group,
#                                  groupnames=groupnames)
# VP$vals %>% 
#   as.data.frame() %>% 
#   kable(format="html") %>% 
#   kable_minimal %>% 
#   scroll_box(height="180px", width="800px")
```
---
# Predictions 
```{r hsmc-ss-prediction1, echo=F, results='hide'}
# m=models100[[1]]
# m$XData$hab %<>% factor()
# Gradient=constructGradient(m,
#                            focalVariable="clim", 
#                            non.focalVariables = list(hab=1))
# predY = predict(m, 
#                 Gradient = Gradient, 
#                 expected = TRUE)
```

```{r hsmc-ss-prediction1-plot, echo=F,fig.align="center", fig.height=8, fig.width=8, message=FALSE}
# par(mfrow=c(1,1))
# plotGradient(m, Gradient, pred = predY, measure = "Y", showData=T)
```

---

# Spatial Prediction 

```{r hmsc-ss-load-pred, echo=F}
# library(abind)
# predY = readRDS("../003_processed_data/hmsc_mcmc/corvusmonedula/prediction.RDS")
# EpredY = apply(abind(predY,along = 3), c(1,2), mean)
# EpredO = apply(
#   abind(predY, along = 3),
#   c(1, 2),
#   FUN = function(a) {
#     mean(a > 0)
#   }
# )
# grid = read.csv(file.path(data.directory,
#                           "grid_1000.csv"))
# grid = droplevels(subset(grid, !(Habitat=="Ma")))
# xy.grid = as.matrix(cbind(grid$x, grid$y))
# mapData=data.frame(xy.grid, EpredY,EpredO)
# names(mapData)=c("xCoordinates", "yCoordinates", "PredictedAbundance", "PredictedOccurence")
# spC <- ggplot(data = mapData, 
#               aes(x= xCoordinates, 
#                   y= yCoordinates, 
#                   color=PredictedAbundance)
# ) +
#   geom_point(size=2)
# spO <- ggplot(data = mapData,
#               aes(x = xCoordinates,
#                   y = yCoordinates,
#                   color = PredictedOccurence)) +
#   geom_point(size = 2)
```

```{r hmsc-ss-pred-fig1, fig.align="center", echo=F, fig.height=8, fig.width=8}
# spO + 
#   ggtitle("Predicted Corvus monedula occurrence") +
#   xlab("East coordinate (km)") + 
#   ylab("North coordinate (km)") + 
#   scale_color_gradient(low = "blue", 
#                        high="red", 
#                        name ="Occurrence probability")
```

---
# Spatial Prediction 
```{r hmsc-ss-pred-fig2, fig.align="center", echo=F,fig.height=8, fig.width=8}
# spC + 
#   ggtitle("Predicted Corvus monedula abundance") +
#   xlab("East coordinate (km)") + 
#   ylab("North coordinate (km)") + 
#   scale_color_gradient(low = "blue", 
#                        high="red", 
#                        name ="Abundance")
```

---

# Joint Species Distribution Modeling with HMSC 

```{r hmsc-jsdm-setup, messsage=F}
# setup ----
# working directory 
setwd(here())
# directories 
dir_data = "001_raw_data/hmsc_birds/data/"
dir_model = "003_processed_data/hmsc_mcmc/jsdm"
# setup ----
dt_y = fread(file.path(dir_data, "data.csv"))
dt_x = dt_y[,c(5,6,7,8,9)]
# load phylogenetic data  
ph_phylo <- ape::read.tree(file.path(dir_data, "CTree.tre"))
# load traits  
dt_traits = fread(file.path(dir_data, "traits.csv")) 
dt_traits$LogMass = log(dt_traits$Mass)
```

---

# Phylogenetic data
```{r phylo-data, fig.align="center"}
ph_phylo
```

---
# Phylogenetic data

```{r hmsc-jsdm-phylo-plot, echo=F, fig.align="center"}
ggtree(ph_phylo, branch.length="none", layout="circular") +
  geom_tiplab(align=TRUE, linesize=.5)
```

---
# Traits
```{r hmsc-jsdm-traits, echo=F}
dt_traits %>% 
  head() %>% 
  kable(format="html") %>% 
  kable_minimal() %>% 
  scroll_box(width="800px")
```

---
# Subset to common species 
```{r hmsc-jsdm-common-species}
vc_subset = sort(colSums(dt_y[,10:59]), decreasing = TRUE)
vc_subset = names(vc_subset)[1:9]
vc_subset = append(vc_subset, "Corvus_monedula")
vc_subset = which(names(dt_y) %in% vc_subset)

dt_y=dt_y[,append(1:9, vc_subset), with = F]
ma_y=as.matrix(dt_y[,10:19])
# reduce trait data to selected species
dt_traits = dt_traits[Species %in% colnames(dt_y)]
```

---
# Study Design 
```{r hmsc-jsdm-study-design}
ma_studydesign           = matrix(NA,nrow(ma_y),2) 
ma_studydesign[,1]       = sprintf('Route_%.3d',dt_y$Route) 
ma_studydesign[,2]       = sprintf('Year_%.3d',dt_y$Year)
df_studydesign           = as.data.frame(ma_studydesign) 
colnames(df_studydesign) = c("Route","Year") 
df_studydesign[,1]       = as.factor(df_studydesign[,1]) 
df_studydesign[,2]       = as.factor(df_studydesign[,2])
```
---
# Study Design

```{r hmsc-jsdm-study-design2, echo=F}
df_studydesign %>% 
  head() %>% 
  kable(format="html") %>% 
  kable_minimal() %>% 
  scroll_box(width="800px")
```
---
# Random Effect Structure 

```{r}
vc_routes  = levels(df_studydesign[,1]) 
ma_xy      = matrix(0, 
                    nrow = length(vc_routes), 
                    ncol = 2)

for (i in seq_along(vc_routes)){ 
        rows       = df_studydesign[,1]==vc_routes[i] 
        ma_xy[i,1] = mean(dt_y[rows,]$x) 
        ma_xy[i,2] = mean(dt_y[rows,]$y)
}

colnames(ma_xy) = c("x","y") 
ma_sRL = ma_xy 
rownames(ma_sRL) = vc_routes 
rL = HmscRandomLevel(sData=ma_sRL) 
rL$nfMin = 5 
rL$nfMax = 10
```


---
# Defining the model 
```{r}

XFormula = ~ Habitat + poly(AprMay, degree = 2, raw = TRUE)
TrFormula = ~Migration + LogMass

m = Hmsc(Y = ma_y, 
         XData = as.data.frame(dt_x), 
         XFormula = XFormula, 
         TrData = dt_traits, 
         TrFormula = TrFormula, 
         phyloTree = ph_phylo, 
         distr = "lognormal poisson", 
         studyDesign = df_studydesign, 
         ranLevels = list(Route=rL))
```
---
# MCMC convergence 

```{r hmsc-jsdm-load-mcmc, echo=F}
m_fit = readRDS("~/01_Uni/01_Lehre/05_MOD3-Lecture/003_processed_data/hmsc_mcmc/jsdm/model_5.RDS")
mpost = convertToCodaObject(m_fit,
                            spNamesNumbers = c(T, F),
                            covNamesNumbers = c(T, F))
```

.scroll-output[
```{r hmsc-jsdm-plot-mcmc, echo=F, fig.align="center", fig.width=8}
plot(mpost$Beta)
```
]
---
.scroll-output[
```{r hmsc-jsdm-plot-mcmc-gamma, echo=F, fig.align="center", fig.width=8}
plot(mpost$Gamma)
```
]
---
.scroll-output[
```{r hmsc-jsdm-plot-mcmc-sigma, echo=F, fig.align="center", fig.width=8}
plot(mpost$Sigma)
```
]

---
# Effective Sample size
```{r hmsc-jsdm-eff, echo=FALSE, fig.align="center"}
es.beta = effectiveSize(mpost$Beta) 
ge.beta = gelman.diag(mpost$Beta,multivariate=FALSE)$psrf
es.gamma = effectiveSize(mpost$Gamma) 
ge.gamma = gelman.diag(mpost$Gamma,multivariate=FALSE)$psrf

es.rho = effectiveSize(mpost$Rho) 
ge.rho = gelman.diag(mpost$Rho,multivariate=FALSE)$psrf 
es.V = effectiveSize(mpost$V) 
ge.V = gelman.diag(mpost$V,multivariate=FALSE)$psrf

es.omega = effectiveSize(mpost$Omega[[1]])
ge.omega = gelman.diag(mpost$Omega[[1]],multivariate=FALSE)$psrf
mixing = list(
        es.beta = es.beta,
        ge.beta = ge.beta,
        es.gamma = es.gamma,
        ge.gamma = ge.gamma,
        es.rho = es.rho,
        ge.rho = ge.rho,
        es.V = es.V,
        ge.V = ge.V,
        es.omega = es.omega,
        ge.omega = ge.omega
)
par(mfrow=c(2,2))
hist(mixing$es.beta)
hist(mixing$ge.beta)
hist(mixing$es.gamma)
hist(mixing$ge.gamma)
```
---
# Evaluate model fit 
```{r hmsc-jsdm-model-fit-read,echo=F}
predY = readRDS("~/01_Uni/01_Lehre/05_MOD3-Lecture/003_processed_data/hmsc_mcmc/jsdm/prediction.RDS")
MF    = readRDS("~/01_Uni/01_Lehre/05_MOD3-Lecture/003_processed_data/hmsc_mcmc/jsdm/model_fit.RDS")
```

---
class: inverse, center, middle 
# Copulas 
---
# What are Copulas? 
- a Copula is a function $C()$ that .blue[couples] marginal distributions

--

- they combine marginal distributions $f_1, ..., f_d$ to a d-dimensional multivariate distribution $h()$

--

- Formally expressed in Sklar's Theorem (Sklar, 1959): 
$$ h(x) = C(f_1(x), ..., f_d(x))$$
--

- the marginals $f_1,...,f_d$ must be standard uniform $U(0,1)$ 

--

- all marginals can be mapped to $U(0,1)$ with probability integral transform (PIT)

---
background-image: url("figures/Anderson2019_1.png")
background-size: 700px
background-position: 50% 80%
# PIT continuous to discrete 
.footnote[Anderson *et al.* (2019)]
---
background-image: url("figures/Anderson2019_2.png")
background-size: 700px
background-position: 50% 80%
# Pit discrete to continuous
.footnote[Anderson *et al.* (2019)]

---

# Examples of Copulas
.content-box-blue[Independence Copula]
 
$$\Pi(\mathbf{u}) = \prod_{j=1}^du_j\ , \ \ u\ \epsilon\ [0,1]^d$$
.content-box-blue[bivariate Frank Copula]  
$$C_\theta^F (\mathbf{u}) = - \frac{1}{\theta}\ \log \bigg(1 + \frac{(exp(-\theta u_1)-1)(exp(-\theta u_2)-1)}{exp(-\theta)-1} \bigg)$$ 
.content-box-blue[Calyton Copula]  
$$C_\theta^C (\mathbf{u}) = max(u_1^{-\theta} + u_2^{-\theta} -1,0 )^{-1/\theta} $$
---
background-image: url("figures/Anderson2019_4.png")
background-size: 500px
background-position: 50% 90%
# What's it good for? 
- disentangle dependence and marginals 
- correlation coefficients are dependent of marginals as dependence structures



.footnote[Anderson *et al.* (2019)]
---
# How can we use it? 
- Roadmap provided by Anderson *et al.* (2019)  

i) identify marginals  

--

ii) Identify significant associations among species to model

--

iii) Fit Copula model 

--

iv) draw random samples from copulas 

---


---
background-image: url("figures/Anderson2019_3.png")
background-size: 700px
background-position: 50% 80%
# Simulate from copulas
.footnote[Anderson *et al.* (2019)]
---
background-image: url("figures/gosh2020b_1.png")
background-size: 700px
background-position: 50% 50%

# Tails associations 

.footnote[Gosh *et al.* (2020b)]
---
background-image: url("figures/gosh2020b_2.png")
background-size: 700px
background-position: 50% 50%

# Tails associations 

.footnote[Gosh *et al.* (2020b)]

---
background-image: url("figures/popovic2019_1.png")
background-size: 500px
background-position: 75% 70%
# Disentangaling species associations

- Popovic *et al.* 2019 and ecoCopula package

- Three reasons for two species to co-occur: 
  + environment
  + mediator species
  + direct association 

.footnote[Popovic *et al.* (2019)]
---
# Gaussian copula graphical models

- graphical models estimate the precision matrix $\Omega$ 

--

- $\Omega = \Sigma^{-1}$

--

- Graphical Models require normal responses

--

- Gaussian Copula models can "transform" to normality
---
# ecoCopula Example
.scroll-box-20[
```{r ecoCopula}
# devtools::install_github("gordy2x/ecoCopula")
# library(ecoCopula)
# gcgm_obj_0 <- cgr(tas.nb,lambda=0)
# plot(gcgm_obj_0)
# gcgm_obj_1 <- cgr(tas.nb,lambda=.1)
# plot(gcgm_obj_1)
# gcgm_obj_2 <- cgr(tas.nb,lambda=.2)
# plot(gcgm_obj_2)
# gcgm_obj_3 <- cgr(tas.nb,lambda=seq(from = 0, to = 0.2,by = 0.01))
# plot(gcgm_obj_3)


```
]
---
class: inverse, center, middle 

# Hierarchical Neural networks 

--- 



